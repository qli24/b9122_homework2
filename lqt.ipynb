{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d6b1aa",
   "metadata": {},
   "source": [
    "# Q1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1b6d787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://press.un.org/en/2023/sgsm21967.doc.htm']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to check if a given page is a press release\n",
    "def is_press_release(soup):\n",
    "    press_release_tag = soup.find('a', {'href': '/en/press-release', 'hreflang': 'en'})\n",
    "    return bool(press_release_tag)\n",
    "\n",
    "# Function to save the HTML source code to a .txt file\n",
    "def save_to_file(content, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Initial seed URL\n",
    "seed_url = \"https://press.un.org/en\"\n",
    "\n",
    "# Fetch and parse the seed page\n",
    "response = requests.get(seed_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all links on the page that might lead to press releases\n",
    "links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].startswith(\"/en/\")]\n",
    "\n",
    "# Counter for the number of press releases found containing the word \"crisis\"\n",
    "counter = 0\n",
    "\n",
    "# List to store press release URLs containing the word \"crisis\"\n",
    "crisis_press_releases = []\n",
    "\n",
    "for link in links:\n",
    "    # Construct full URL\n",
    "    full_url = \"https://press.un.org\" + link\n",
    "\n",
    "    # Fetch and parse the press release page\n",
    "    response = requests.get(full_url)\n",
    "    press_release_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Check if the page is a press release by looking for the \"PRESS RELEASE\" link\n",
    "    if is_press_release(press_release_soup):\n",
    "        # Check if the word \"crisis\" exists in the press release\n",
    "        if \"crisis\" in press_release_soup.get_text().lower():\n",
    "            crisis_press_releases.append(full_url)\n",
    "            counter += 1\n",
    "\n",
    "            # Save the complete HTML source code to a .txt file\n",
    "            save_to_file(response.text, f\"1_{counter}.txt\")\n",
    "\n",
    "    # Stop if we've found 10 press releases containing the word \"crisis\"\n",
    "    if counter >= 10:\n",
    "        break\n",
    "\n",
    "print(crisis_press_releases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba5697",
   "metadata": {},
   "source": [
    "# Q1-1 bfs ☑️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5208daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://press.un.org\"\n",
    "\n",
    "# Function to check if a given page is a press release\n",
    "def is_press_release(soup):\n",
    "    press_release_tag = soup.find('a', {'href': '/en/press-release', 'hreflang': 'en'})\n",
    "    return bool(press_release_tag)\n",
    "\n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    time.sleep(1)  # Pause for 1 second after each request\n",
    "    return BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "def bfs_crawl(start_url):\n",
    "    visited = set()  # To keep track of visited URLs\n",
    "    queue = deque([start_url])  # Queue initialized with the start URL\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    while queue:\n",
    "        url = queue.popleft()\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        soup = get_soup(url)\n",
    "        visited.add(url)\n",
    "\n",
    "        if is_press_release(soup) and \"crisis\" in soup.get_text().lower():\n",
    "            counter += 1\n",
    "            # Save the content to a file\n",
    "            with open(f\"1_{counter}.txt\", 'w', encoding='utf-8') as f:\n",
    "                f.write(soup.prettify())\n",
    "            # Save the URL to crisis.txt\n",
    "            with open(\"crisis_1.txt\", 'a', encoding='utf-8') as f:\n",
    "                f.write(url + '\\n')\n",
    "            \n",
    "            if counter >= 10:\n",
    "                break\n",
    "\n",
    "        # Extract links to enqueue\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if href.startswith(('#', 'javascript:', 'mailto:')):\n",
    "                continue  # Skip invalid links\n",
    "\n",
    "            # Properly join the base URL with the href\n",
    "            if href.startswith('?'):\n",
    "                full_url = url + href\n",
    "            elif href.startswith(\"/\"):\n",
    "                full_url = BASE_URL + href\n",
    "            else:\n",
    "                full_url = href\n",
    "\n",
    "            if full_url not in visited:\n",
    "                queue.append(full_url)\n",
    "\n",
    "bfs_crawl(\"https://press.un.org/en\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86f482f",
   "metadata": {},
   "source": [
    "# Q1-1 scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db99d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scrapy\n",
    "# 安装确保在 Jupyter 环境中安装了 Scrapy。如果没有，可以使用 !pip install scrapy 进行安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a0b40c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.exceptions import CloseSpider\n",
    "\n",
    "# Define the spider class\n",
    "class UNPressSpider(scrapy.Spider):\n",
    "    name = 'un_press_releases'\n",
    "    start_urls = ['https://press.un.org/en']\n",
    "\n",
    "    # Counter for the number of press releases found containing the word \"crisis\"\n",
    "    count = 0\n",
    "    MAX_COUNT = 10\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Check if the current page is a press release\n",
    "        if response.css('a[href=\"/en/press-release\"][hreflang=\"en\"]'):\n",
    "            \n",
    "            # Check if the word \"crisis\" exists in the press release\n",
    "            if \"crisis\" in response.text.lower():\n",
    "                self.count += 1\n",
    "                # Save the content to a file\n",
    "                with open(f\"1_{self.count}.txt\", 'w', encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "                \n",
    "                # Save the URL to crisis.txt\n",
    "                with open(\"crisis_1.txt\", 'a', encoding='utf-8') as f:\n",
    "                    f.write(response.url + '\\n')\n",
    "                \n",
    "                if self.count >= self.MAX_COUNT:\n",
    "                    raise CloseSpider('Reached maximum count of press releases containing \"crisis\".')\n",
    "\n",
    "        # Extract all links and recursively scrape them\n",
    "        for link in response.css('a::attr(href)').extract():\n",
    "            if link.startswith('/en/') and 'javascript:' not in link and not link.startswith('mailto:'):\n",
    "                yield response.follow(link, self.parse)\n",
    "\n",
    "\n",
    "\n",
    "# Configure settings and run the spider\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/5.0',\n",
    "    'LOG_LEVEL': 'ERROR',  # Change to 'INFO' to see more logs\n",
    "})\n",
    "\n",
    "process.crawl(UNPressSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618cb5a",
   "metadata": {},
   "source": [
    "# Q1-2\n",
    "跳过部分网站"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd27a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://www.europarl.europa.eu/news/en/press-room\"\n",
    "\n",
    "def is_plenary_session_press_release(soup):\n",
    "    return soup.find('span', class_='ep_name', string='Plenary session') is not None\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        time.sleep(1)  # Pause for 1 second after each request\n",
    "        response.raise_for_status()  # raise HTTPError for bad responses (4xx and 5xx)\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        return None  # or you can log the error or do something else\n",
    "\n",
    "def bfs_crawl(start_url):\n",
    "    visited = set()  # To keep track of visited URLs\n",
    "    queue = deque([start_url])  # Queue initialized with the start URL\n",
    "    counter = 0\n",
    "\n",
    "    while queue:\n",
    "        url = queue.popleft()\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        soup = get_soup(url)\n",
    "        if soup is None:  # Check if soup is None\n",
    "            continue  # Skip this URL\n",
    "        \n",
    "        visited.add(url)\n",
    "\n",
    "        if is_plenary_session_press_release(soup) and \"crisis\" in soup.get_text().lower():\n",
    "            counter += 1\n",
    "            with open(f\"2_{counter}.txt\", 'w', encoding='utf-8') as f:\n",
    "                f.write(soup.prettify())\n",
    "            with open(\"crisis_2.txt\", 'a', encoding='utf-8') as f:\n",
    "                f.write(url + '\\n')\n",
    "            \n",
    "            if counter >= 10:\n",
    "                break\n",
    "\n",
    "         # Extract links to enqueue\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if not href or href.startswith(('#', 'javascript:', 'mailto:')):\n",
    "                continue  # Skip invalid or empty links\n",
    "\n",
    "            # Properly join the base URL with the href\n",
    "            if href.startswith('?'):\n",
    "                full_url = url + href\n",
    "            elif href.startswith(\"/\"):\n",
    "                full_url = BASE_URL + href\n",
    "            else:\n",
    "                full_url = href\n",
    "\n",
    "            if full_url not in visited:\n",
    "                queue.append(full_url)\n",
    "\n",
    "bfs_crawl(\"https://www.europarl.europa.eu/news/en/press-room\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d97a6",
   "metadata": {},
   "source": [
    "# Q1-2 优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d07cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://www.europarl.europa.eu/news/en/press-room\"\n",
    "\n",
    "# 使用namedtuple存储URL和其深度\n",
    "URLData = namedtuple('URLData', ['url', 'depth'])\n",
    "\n",
    "cache = {}  # 用于缓存页面内容的字典\n",
    "\n",
    "def is_plenary_session_press_release(soup):\n",
    "    return soup.find('span', class_='ep_name', string='Plenary session') is not None\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        time.sleep(1)  # Pause for 1 second after each request\n",
    "        response.raise_for_status()  # raise HTTPError for bad responses (4xx and 5xx)\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        return None  # or you can log the error or do something else\n",
    "\n",
    "def bfs_crawl(start_url, max_depth=3):\n",
    "    visited = set()\n",
    "    queue = deque([URLData(start_url, 0)])  # 使用namedtuple初始化队列\n",
    "    counter = 0\n",
    "\n",
    "    while queue:\n",
    "        url_data = queue.popleft()\n",
    "        url, depth = url_data.url, url_data.depth\n",
    "\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        # 使用缓存，避免重复下载页面\n",
    "        if url not in cache:\n",
    "            soup = get_soup(url)\n",
    "            if soup is None:  # Check if soup is None\n",
    "                continue  # Skip this URL\n",
    "            if soup:\n",
    "                cache[url] = soup\n",
    "        else:\n",
    "            soup = cache[url]\n",
    "        \n",
    "        visited.add(url)\n",
    "\n",
    "        if is_plenary_session_press_release(soup) and \"crisis\" in soup.get_text().lower():\n",
    "            counter += 1\n",
    "            # Save the content to a file\n",
    "            with open(f\"2_{counter}.txt\", 'w', encoding='utf-8') as f:\n",
    "                f.write(soup.prettify())\n",
    "            # Save the URL to crisis.txt\n",
    "            with open(\"crisis_2.txt\", 'a', encoding='utf-8') as f:\n",
    "                f.write(url + '\\n')\n",
    "            \n",
    "            if counter >= 10:\n",
    "                break\n",
    "\n",
    "        # 如果未达到最大深度，继续爬取\n",
    "        if depth < max_depth:\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if not href or href.startswith(('#', 'javascript:', 'mailto:')):\n",
    "                    continue  # Skip invalid or empty links\n",
    "\n",
    "                # Properly join the base URL with the href\n",
    "                if href.startswith('?'):\n",
    "                    full_url = url + href\n",
    "                elif href.startswith(\"/\"):\n",
    "                    full_url = BASE_URL + href\n",
    "                else:\n",
    "                    full_url = href\n",
    "\n",
    "                if full_url not in visited:\n",
    "                    queue.append(URLData(full_url, depth+1))  # 增加深度\n",
    "\n",
    "bfs_crawl(\"https://www.europarl.europa.eu/news/en/press-room\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971bfeec",
   "metadata": {},
   "source": [
    "# Q1-2 V2.0 优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3144814c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests_cache\n",
      "  Downloading requests_cache-1.1.0-py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 376 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting platformdirs>=2.5\n",
      "  Downloading platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: urllib3>=1.25.5 in /Users/eyan/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests_cache) (1.26.7)\n",
      "Requirement already satisfied: requests>=2.22 in /Users/eyan/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests_cache) (2.31.0)\n",
      "Collecting url-normalize>=1.4\n",
      "  Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
      "Collecting cattrs>=22.2\n",
      "  Downloading cattrs-23.1.2-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=21.2 in /Users/eyan/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests_cache) (21.4.0)\n",
      "Collecting exceptiongroup\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Collecting typing_extensions>=4.1.0\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/eyan/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests>=2.22->requests_cache) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/eyan/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests>=2.22->requests_cache) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/eyan/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests>=2.22->requests_cache) (3.3)\n",
      "Requirement already satisfied: six in /Users/eyan/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages (from url-normalize>=1.4->requests_cache) (1.16.0)\n",
      "Installing collected packages: typing-extensions, exceptiongroup, url-normalize, platformdirs, cattrs, requests-cache\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "Successfully installed cattrs-23.1.2 exceptiongroup-1.1.3 platformdirs-3.11.0 requests-cache-1.1.0 typing-extensions-4.8.0 url-normalize-1.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install requests_cache==0.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97afee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# 设置缓存\n",
    "requests_cache.install_cache()\n",
    "\n",
    "BASE_URL = \"https://www.europarl.europa.eu/news/en/press-room\"\n",
    "\n",
    "URLData = namedtuple('URLData', ['url', 'depth'])\n",
    "\n",
    "def is_plenary_session_press_release(soup):\n",
    "    return soup.find('span', class_='ep_name', string='Plenary session') is not None\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "def bfs_crawl(start_url, max_depth=3):\n",
    "    visited = set()\n",
    "    queue = deque([URLData(start_url, 0)])\n",
    "    counter = 0\n",
    "\n",
    "    while queue:\n",
    "        url_data = queue.popleft()\n",
    "        url, depth = url_data.url, url_data.depth\n",
    "\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        soup = get_soup(url)\n",
    "        if soup is None:  # Check if soup is None\n",
    "            continue  # Skip this URL if soup is None\n",
    "        visited.add(url)\n",
    "\n",
    "        if is_plenary_session_press_release(soup) and \"crisis\" in soup.get_text().lower():\n",
    "            counter += 1\n",
    "            with open(f\"2_{counter}.txt\", 'w', encoding='utf-8') as f:\n",
    "                f.write(soup.prettify())\n",
    "            with open(\"crisis_2.txt\", 'a', encoding='utf-8') as f:\n",
    "                f.write(url + '\\n')\n",
    "            \n",
    "            if counter >= 10:\n",
    "                break\n",
    "\n",
    "        if depth < max_depth:\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if not href or href.startswith(('#', 'javascript:', 'mailto:')):\n",
    "                    continue\n",
    "\n",
    "                if href.startswith('?'):\n",
    "                    full_url = url + href\n",
    "                elif href.startswith(\"/\"):\n",
    "                    full_url = BASE_URL + href\n",
    "                else:\n",
    "                    full_url = href\n",
    "\n",
    "                if full_url not in visited:\n",
    "                    queue.append(URLData(full_url, depth+1))\n",
    "\n",
    "bfs_crawl(\"https://www.europarl.europa.eu/news/en/press-room\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4480d",
   "metadata": {},
   "source": [
    "# Q1-2 使用requests库来模拟发送带有搜索关键字的GET请求 V1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# 设置缓存\n",
    "requests_cache.install_cache()\n",
    "\n",
    "BASE_URL = \"https://www.europarl.europa.eu/news/en/press-room\"\n",
    "SEARCH_URL = BASE_URL  # 使用相同的URL进行搜索\n",
    "\n",
    "URLData = namedtuple('URLData', ['url', 'depth'])\n",
    "\n",
    "def is_plenary_session_press_release(soup):\n",
    "    return soup.find('span', class_='ep_name', string='Plenary session') is not None\n",
    "\n",
    "def get_soup(url, params=None):\n",
    "    headers = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.9\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Host\": \"www.europarl.europa.eu\",\n",
    "        \"If-Modified-Since\": \"Fri, 06 Oct 2023 16:26:42 UTC\",\n",
    "        \"If-None-Match\": '\"0da04c545a722ed9e772a81e630581bf5\"',\n",
    "        \"Referer\": \"https://www.europarl.europa.eu/news/en/press-room\",\n",
    "        \"Sec-Ch-Ua\": '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "        \"Sec-Ch-Ua-Platform\": \"macOS\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"same-origin\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "def bfs_crawl(search_keyword, max_depth=1):\n",
    "    visited = set()\n",
    "    counter = 0\n",
    "\n",
    "    # 使用搜索关键字发送GET请求\n",
    "    soup = get_soup(SEARCH_URL, params={'searchQuery': search_keyword})\n",
    "    if soup is None:\n",
    "        print(\"Failed to get the search results.\")\n",
    "        return\n",
    "\n",
    "    # 基于搜索结果初始化队列\n",
    "    queue = deque([URLData(SEARCH_URL, 0)])\n",
    "\n",
    "    while queue:\n",
    "        url_data = queue.popleft()\n",
    "        url, depth = url_data.url, url_data.depth\n",
    "\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        soup = get_soup(url)\n",
    "        if soup is None:\n",
    "            continue\n",
    "\n",
    "        visited.add(url)\n",
    "\n",
    "        if is_plenary_session_press_release(soup) and \"crisis\" in soup.get_text().lower():\n",
    "            counter += 1\n",
    "            with open(f\"2_{counter}.txt\", 'w', encoding='utf-8') as f:\n",
    "                f.write(soup.prettify())\n",
    "            with open(\"crisis_2.txt\", 'a', encoding='utf-8') as f:\n",
    "                f.write(url + '\\n')\n",
    "            \n",
    "            if counter >= 10:\n",
    "                break\n",
    "\n",
    "        if depth < max_depth:\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if not href or href.startswith(('#', 'javascript:', 'mailto:')):\n",
    "                    continue\n",
    "\n",
    "                if href.startswith('?'):\n",
    "                    full_url = url + href\n",
    "                elif href.startswith(\"/\"):\n",
    "                    full_url = BASE_URL + href\n",
    "                else:\n",
    "                    full_url = href\n",
    "\n",
    "                if full_url not in visited:\n",
    "                    queue.append(URLData(full_url, depth+1))\n",
    "\n",
    "bfs_crawl(\"crisis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eed86c",
   "metadata": {},
   "source": [
    "# Q1-2 使用requests库来模拟发送带有搜索关键字的GET请求 V2.0 ☑️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e9f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests_cache==0.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48ca8fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# 设置缓存\n",
    "requests_cache.install_cache()\n",
    "\n",
    "BASE_URL = \"https://www.europarl.europa.eu/news/en/press-room\"\n",
    "SEARCH_URL = BASE_URL  # 使用相同的URL进行搜索\n",
    "\n",
    "URLData = namedtuple('URLData', ['url', 'depth'])\n",
    "\n",
    "def is_plenary_session_press_release(soup):\n",
    "    return soup.find('span', class_='ep_name', string='Plenary session') is not None\n",
    "\n",
    "def get_soup(url, params=None):\n",
    "    headers = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.9\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Host\": \"www.europarl.europa.eu\",\n",
    "        \"If-Modified-Since\": \"Fri, 06 Oct 2023 16:26:42 UTC\",\n",
    "        \"If-None-Match\": '\"0da04c545a722ed9e772a81e630581bf5\"',\n",
    "        \"Referer\": \"https://www.europarl.europa.eu/news/en/press-room\",\n",
    "        \"Sec-Ch-Ua\": '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "        \"Sec-Ch-Ua-Platform\": \"macOS\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"same-origin\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "\n",
    "def bfs_crawl(search_keyword, max_depth=1, max_pages=3):\n",
    "    visited = set()\n",
    "    counter = 0\n",
    "\n",
    "    # 使用搜索关键字发送GET请求\n",
    "    soup = get_soup(SEARCH_URL, params={'searchQuery': search_keyword})\n",
    "    if soup is None:\n",
    "        print(\"Failed to get the search results.\")\n",
    "        return\n",
    "\n",
    "    # 基于搜索结果初始化队列\n",
    "    queue = deque([URLData(SEARCH_URL, 0)])\n",
    "\n",
    "    # 将前max_pages个页面添加到队列\n",
    "    for page in range(1, max_pages+1):\n",
    "        page_url = f\"{BASE_URL}/page/{page}?searchQuery={search_keyword}\"\n",
    "        queue.append(URLData(page_url, 0))\n",
    "\n",
    "    while queue:\n",
    "        url_data = queue.popleft()\n",
    "        url, depth = url_data.url, url_data.depth\n",
    "\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        soup = get_soup(url)\n",
    "        if soup is None:\n",
    "            continue\n",
    "\n",
    "        visited.add(url)\n",
    "\n",
    "        if is_plenary_session_press_release(soup) and \"crisis\" in soup.get_text().lower():\n",
    "            counter += 1\n",
    "            with open(f\"2_{counter}.txt\", 'w', encoding='utf-8') as f:\n",
    "                f.write(soup.prettify())\n",
    "            with open(\"crisis_2.txt\", 'a', encoding='utf-8') as f:\n",
    "                f.write(url + '\\n')\n",
    "            \n",
    "            if counter >= 10:\n",
    "                break\n",
    "\n",
    "        if depth < max_depth:\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if not href or href.startswith(('#', 'javascript:', 'mailto:')):\n",
    "                    continue\n",
    "\n",
    "                if href.startswith('?'):\n",
    "                    full_url = url + href\n",
    "                elif href.startswith(\"/\"):\n",
    "                    full_url = BASE_URL + href\n",
    "                else:\n",
    "                    full_url = href\n",
    "\n",
    "                if full_url not in visited:\n",
    "                    queue.append(URLData(full_url, depth+1))\n",
    "\n",
    "bfs_crawl(\"crisis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b7cf3",
   "metadata": {},
   "source": [
    "# Q1-2 暴力完整的搜索URL，直接开始从这个URL爬取 https://www.europarl.europa.eu/news/en/press-room?searchQuery=crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "BASE_URL = \"https://www.europarl.europa.eu/news/en/press-room\"\n",
    "SEARCH_URL = \"https://www.europarl.europa.eu/news/en/press-room?searchQuery=crisis\"\n",
    "\n",
    "URLData = namedtuple('URLData', ['url', 'depth'])\n",
    "\n",
    "HEADERS = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Host\": \"www.europarl.europa.eu\",\n",
    "    \"Referer\": \"https://www.europarl.europa.eu/news/en/press-room\",\n",
    "    \"Sec-Ch-Ua\": '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "    \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "    \"Sec-Ch-Ua-Platform\": \"macOS\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    \"Sec-Fetch-User\": \"?1\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def is_plenary_session_press_release(soup):\n",
    "    return soup.find('span', class_='ep_name', string='Plenary session') is not None\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "def bfs_crawl(start_url, max_depth=2):\n",
    "    visited = set()\n",
    "    queue = deque([URLData(start_url, 0)])\n",
    "    counter = 0\n",
    "\n",
    "    while queue:\n",
    "        url_data = queue.popleft()\n",
    "        url, depth = url_data.url, url_data.depth\n",
    "\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        soup = get_soup(url)\n",
    "        if soup is None:\n",
    "            continue\n",
    "\n",
    "        visited.add(url)\n",
    "\n",
    "        if is_plenary_session_press_release(soup) and \"crisis\" in soup.get_text().lower():\n",
    "            counter += 1\n",
    "            with open(f\"2_{counter}.txt\", 'w', encoding='utf-8') as f:\n",
    "                f.write(soup.prettify())\n",
    "            with open(\"crisis_2.txt\", 'a', encoding='utf-8') as f:\n",
    "                f.write(url + '\\n')\n",
    "            \n",
    "            if counter >= 12:\n",
    "                break\n",
    "\n",
    "        if depth < max_depth:\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if not href or href.startswith(('#', 'javascript:', 'mailto:')):\n",
    "                    continue\n",
    "\n",
    "                if href.startswith('?'):\n",
    "                    full_url = url + href\n",
    "                elif href.startswith(\"/\"):\n",
    "                    full_url = BASE_URL + href\n",
    "                else:\n",
    "                    full_url = href\n",
    "\n",
    "                if full_url not in visited:\n",
    "                    queue.append(URLData(full_url, depth+1))\n",
    "\n",
    "bfs_crawl(SEARCH_URL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
