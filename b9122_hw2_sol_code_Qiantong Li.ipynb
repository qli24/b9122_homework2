{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef2ff3d",
   "metadata": {},
   "source": [
    "# b9122_hw2_sol_code_Qiantong Li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf325da2",
   "metadata": {},
   "source": [
    "# Q1-1 bs4 (Code 1- related more to class- plz ignore XML Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a81b4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (4.8.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fde0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://press.un.org\"\n",
    "\n",
    "# Function to check if a given page is a press release\n",
    "def is_press_release(soup):\n",
    "    press_release_tag = soup.find('a', {'href': '/en/press-release', 'hreflang': 'en'})\n",
    "    return bool(press_release_tag)\n",
    "\n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    # time.sleep(1)  # Pause for 1 second after each request\n",
    "    return BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "def bfs_crawl(start_url):\n",
    "    visited = set()  # To keep track of visited URLs\n",
    "    queue = deque([start_url])  # Queue initialized with the start URL\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    while queue:\n",
    "        url = queue.popleft()\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        soup = get_soup(url)\n",
    "        visited.add(url)\n",
    "\n",
    "        if is_press_release(soup) and \"crisis\" in soup.get_text().lower():\n",
    "            counter += 1\n",
    "            # Save the content to a file\n",
    "            with open(f\"1_{counter}.txt\", 'w', encoding='utf-8') as f:\n",
    "                f.write(soup.prettify())\n",
    "            # Save the URL to crisis.txt\n",
    "            with open(\"crisis_1.txt\", 'a', encoding='utf-8') as f:\n",
    "                f.write(url + '\\n')\n",
    "\n",
    "            if counter >= 12:\n",
    "                break\n",
    "\n",
    "        # Extract links to enqueue\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if href.startswith(('#', 'javascript:', 'mailto:')):\n",
    "                continue  # Skip invalid links\n",
    "\n",
    "            # Properly join the base URL with the href\n",
    "            if href.startswith('?'):\n",
    "                full_url = url + href\n",
    "            elif href.startswith(\"/\"):\n",
    "                full_url = BASE_URL + href\n",
    "            else:\n",
    "                full_url = href\n",
    "\n",
    "            if full_url not in visited:\n",
    "                queue.append(full_url)\n",
    "\n",
    "bfs_crawl(\"https://press.un.org/en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49694662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d972b7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53928f79",
   "metadata": {},
   "source": [
    "# Q1-1 scrapy (Code 2-running more smoothly on my computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a940ccb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (2.6.1)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (1.0.4)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (21.0.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (1.21.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (22.2.0)\n",
      "Requirement already satisfied: tldextract in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (3.2.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (3.4.8)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (0.3.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: protego>=0.1.15 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: setuptools in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (61.2.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from scrapy) (4.8.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.21)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Requirement already satisfied: six>=1.6.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from parsel>=1.5.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from service-identity>=16.0.0->scrapy) (21.4.0)\n",
      "Requirement already satisfied: pyasn1 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: constantly>=15.1 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from Twisted>=17.9.0->scrapy) (4.1.1)\n",
      "Requirement already satisfied: incremental>=21.3.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from Twisted>=17.9.0->scrapy) (21.3.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from Twisted>=17.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: idna>=2.5 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (3.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: requests>=2.1.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from tldextract->scrapy) (2.27.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from tldextract->scrapy) (3.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.1.0->tldextract->scrapy) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b968ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.exceptions import CloseSpider\n",
    "\n",
    "# Define the spider class\n",
    "class UNPressSpider(scrapy.Spider):\n",
    "    name = 'un_press_releases'\n",
    "    start_urls = ['https://press.un.org/en']\n",
    "\n",
    "    # Counter for the number of press releases found containing the word \"crisis\"\n",
    "    count = 0\n",
    "    MAX_COUNT = 10\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Check if the current page is a press release\n",
    "        if response.css('a[href=\"/en/press-release\"][hreflang=\"en\"]'):\n",
    "            \n",
    "            # Check if the word \"crisis\" exists in the press release\n",
    "            if \"crisis\" in response.text.lower():\n",
    "                self.count += 1\n",
    "                # Save the content to a file\n",
    "                with open(f\"1_{self.count}.txt\", 'w', encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "                \n",
    "                # Save the URL to crisis.txt\n",
    "                with open(\"crisis_1.txt\", 'a', encoding='utf-8') as f:\n",
    "                    f.write(response.url + '\\n')\n",
    "                \n",
    "                if self.count >= self.MAX_COUNT:\n",
    "                    raise CloseSpider('Reached maximum count of press releases containing \"crisis\".')\n",
    "\n",
    "        # Extract all links and recursively scrape them\n",
    "        for link in response.css('a::attr(href)').extract():\n",
    "            if link.startswith('/en/') and 'javascript:' not in link and not link.startswith('mailto:'):\n",
    "                yield response.follow(link, self.parse)\n",
    "\n",
    "\n",
    "\n",
    "# Configure settings and run the spider\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/5.0',\n",
    "    'LOG_LEVEL': 'ERROR',  # Change to 'INFO' to see more logs\n",
    "})\n",
    "\n",
    "process.crawl(UNPressSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b234b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c97a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73ac33f5",
   "metadata": {},
   "source": [
    "# Q1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1893b8b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests_cache==0.5.2 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (0.5.2)\n",
      "Requirement already satisfied: requests>=1.1.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from requests_cache==0.5.2) (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from requests>=1.1.0->requests_cache==0.5.2) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from requests>=1.1.0->requests_cache==0.5.2) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from requests>=1.1.0->requests_cache==0.5.2) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jli/opt/anaconda3/lib/python3.9/site-packages (from requests>=1.1.0->requests_cache==0.5.2) (1.26.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests_cache==0.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee521ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# 设置缓存 Set up caching\n",
    "requests_cache.install_cache()\n",
    "\n",
    "BASE_URL = \"https://www.europarl.europa.eu/news/en/press-room\"\n",
    "SEARCH_URL = BASE_URL  # 使用相同的URL进行搜索 Perform a search with the same URL\n",
    "\n",
    "URLData = namedtuple('URLData', ['url', 'depth'])\n",
    "\n",
    "def is_plenary_session_press_release(soup):\n",
    "    return soup.find('span', class_='ep_name', string='Plenary session') is not None\n",
    "\n",
    "def get_soup(url, params=None):\n",
    "    headers = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.9\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Host\": \"www.europarl.europa.eu\",\n",
    "        \"If-Modified-Since\": \"Fri, 06 Oct 2023 16:26:42 UTC\",\n",
    "        \"If-None-Match\": '\"0da04c545a722ed9e772a81e630581bf5\"',\n",
    "        \"Referer\": \"https://www.europarl.europa.eu/news/en/press-room\",\n",
    "        \"Sec-Ch-Ua\": '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "        \"Sec-Ch-Ua-Platform\": \"macOS\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"same-origin\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "\n",
    "def bfs_crawl(search_keyword, max_depth=1, max_pages=3):\n",
    "    visited = set()\n",
    "    counter = 0\n",
    "\n",
    "    # 使用搜索关键字发送GET请求 Send a GET request using search keywords\n",
    "    soup = get_soup(SEARCH_URL, params={'searchQuery': search_keyword})\n",
    "    if soup is None:\n",
    "        print(\"Failed to get the search results.\")\n",
    "        return\n",
    "\n",
    "    # 基于搜索结果初始化队列 Initialize a queue based on search results\n",
    "    queue = deque([URLData(SEARCH_URL, 0)])\n",
    "\n",
    "    # 将前max_pages个页面添加到队列 Add the first max_pages pages to the queue\n",
    "    for page in range(1, max_pages+1):\n",
    "        page_url = f\"{BASE_URL}/page/{page}?searchQuery={search_keyword}\"\n",
    "        queue.append(URLData(page_url, 0))\n",
    "\n",
    "    while queue:\n",
    "        url_data = queue.popleft()\n",
    "        url, depth = url_data.url, url_data.depth\n",
    "\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        soup = get_soup(url)\n",
    "        if soup is None:\n",
    "            continue\n",
    "\n",
    "        visited.add(url)\n",
    "\n",
    "        if is_plenary_session_press_release(soup) and \"crisis\" in soup.get_text().lower():\n",
    "            counter += 1\n",
    "            with open(f\"2_{counter}.txt\", 'w', encoding='utf-8') as f:\n",
    "                f.write(soup.prettify())\n",
    "            with open(\"crisis_2.txt\", 'a', encoding='utf-8') as f:\n",
    "                f.write(url + '\\n')\n",
    "            \n",
    "            if counter >= 15:\n",
    "                break\n",
    "\n",
    "        if depth < max_depth:\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if not href or href.startswith(('#', 'javascript:', 'mailto:')):\n",
    "                    continue\n",
    "\n",
    "                if href.startswith('?'):\n",
    "                    full_url = url + href\n",
    "                elif href.startswith(\"/\"):\n",
    "                    full_url = BASE_URL + href\n",
    "                else:\n",
    "                    full_url = href\n",
    "\n",
    "                if full_url not in visited:\n",
    "                    queue.append(URLData(full_url, depth+1))\n",
    "\n",
    "bfs_crawl(\"crisis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d40c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
